# Things Are People Too
## Negotiating privacy with voice assistants - An analysis of our relationship to the internet of things

The internet of things, IoT for short, consists of things that talk to each other through the help of sensors, software, and data exchanges. Voice-activated interfaces, as seen in voice assistants devices like the Amazon Echo, are a highly humanized way of interacting with the internet of things.

Despite building on a very human way of interaction, namely oral communication, there are many issues to be found in this approach to interface design as this research project uncovered. I argue that these problems can't be solved by improving the technology alone, but that they also stem from being a specifically disembodied implementation of interaction and thus need to be improved through the approach of user experience design.

Originally this project was interested in introducing animistic practices, as they are found in non-western epistemologies [@guthTheorizingHariKuyo2014], into the interaction with everyday technologies[^1]. As the ethnographic research done during the last year could not unearth substantial material to back up this initial approach, the hypothesis as well as the accompanying research questions needed to be reassessed.

This paper is accompanied by a presentation to be held in June 2021, which will expand on the findings, the underlying framework as well as the concept behind the upcoming phase of prototyping. I also invite you to explore the research blog which contains personal insights, through the endnotes in this paper.

## Process
During the data gathering process, I was mainly interested in the emotional bond between users and their voice assistants. How these bonds form, what enables them, how they are present in the interaction and behaviors. I found guidance for this approach in emotionally durable design by Jonathan Chapman, a practical framework that is used in industrial design [@haines-gaddEmotionalDurabilityDesign2018]. The second important point was the capturing of weak signals. These are a concept from speculative or future design and can be thought of as micro trends. The weak signals, pain points but also memorable experiences, would go into the prototyping of a voice assistant that improves on the current implementations on the market, according to the values formulated by this project.

I gathered insights and data from three different main sources. I led interviews[^2] with owners of voice assistants and could conduct around 25 hours of participatory observation[^3] of interaction, as well as a handful of unboxing and setup user journeys[^4]. The second source was user experience reports that owners of voice assistants posted online. I also researched the image worlds of hacked and customized devices as well as the advertisement created by the brands, although these two assets were not considered in the analysis of the data.

The third and last source of insights was collected through an expert workshop with designers from different disciplines. I wanted to have the opinion of specialists in their respective areas on how to improve the emotional bond between users and their voice assistant devices. The workshop was based on the emotional durable design framework as well as the collected data. The methodology of the framework oriented itself along the lines of speculative design. I wanted to push the participating designers towards imagining alternative ways of interacting or deploying voice user interfaces.[^5]

All the recordings and field notes were transcribed, anonymized, and pushed into a public archive, together with other media, like photos or documents. To work with the collected data I opted for thematic analysis, with a reflexive approach after Braun and Clarke. This is a rather classical qualitative data analysis approach that labels the transcribed data and then builds overarching themes out of the codes.

> The reflexivity process can be described as the researcher reflecting on and documenting how their values, positionings, choices, and research practices influenced and shaped the study and the final analysis of the data. [@braunThematicAnalysis2019]

I went for this approach, as it felt intuitive and spoke to me in terms of my values as research as an embodied practice. Themes do not emerge passively out of the data by themselves but are actively created by me, the researching person. [^6] [^7] 

Next to the fieldwork and the thematic analysis, I had an ongoing process of literature study, exchange with scholars from the field of design theory as well as an attempt to formulate an underlying framework upon which I could base the prototyping phase. Within this process I was able to present my project at two different conferences; Reclaim Futures in September 2020[^8] and the very giving NERD in June 2021[^9].

## Findings
The reassessed hypothesis presented at the beginning of this paper is based on the thematic analysis as well as direct insights by my participants. Through the process of analysis, I could read two main themes out of the collected data. [^10]

### Uncanny Valley
The first theme can be subsumed under the concept of the uncanny valley. This is a psychological effect in which the more a thing is perceived as human the more trustworthy or relatable it is. In this linear progress, there is a brief gap of cognitive dissonance when we can't decide anymore if something is human or not. This gap is the uncanny valley, and we generally don't trust things if they are to be placed in there. [@moriUncannyValley2012]

The uncanny valley finds many expressions in the interaction with voice assistants. First, we have **uncanny communication** by encountering human voices but robotic speech patterns. The voices of current implements are near perfect, indistinguishable from real humans, but the way they communicate is perceived as robotic or scripted.

Another often encountered problem is that the devices go off without users intending them to do so; **activation without interaction**. I have more than one funny and sometimes downwards creepy anecdote to tell around this. These seemingly random activations are often not perceived as quirky, like with a pet, but alienating.

The last issue is that voice assistants build up a **ghostly presence**. This perception arises from the fact, that voice assistants are always-on devices. They are always listening in on their environment, waiting for a wake word. Upon hearing which they unlock and are available for further interaction. Knowing this made my participants feel a presence in the room that led them to alter their behaviors and how they spoke.

### Trust Issues
The second theme covers trust issues, that my participants had with their voice assistants. This theme can be broken down into **issues with privacy, intimacy, and consent**.

The problems around **privacy** hover mainly around the data tracking habits of issuing companies like Google or Amazon. Most of my participants were acutely aware of these practices. They either took this as the price they have to pay or were considering changing providers if a privacy-sensitive alternative would exist.

Generally, these devices are placed within intimate contexts of one's own life. On the other hand, voice assistants are invisible interfaces that withdraw from the negotiation of **intimacy**. This withdrawal is partially intended by the manufactures, argues Adam Greenfield [@greenfieldRadicalTechnologiesDesign2018], in order for the devices to colonize our homes. The problem boils down to us inviting these devices into our innermost intimacies, while not offering anything similar in return.

Adding to these two points is the fact, that users have little to no control over how they want to interact with the voice assistants. Users have no way of giving **consent** to many of the processes that are run by the device.

The general problem that arises out of these two themes would then be as follows.

> Users of voice assistants miss a proper vocabulary to deal with the other-than-human presence, especially in case of errors, and generally have a hard time bonding with voice assistants. They are further unable to deal with issues of trust and can assert only little control over the negotiation of these important aspects.

It seems that neither the user nor the device knows enough about the other to enable communication and interaction that would lead to trust and bonding. The interaction with a voice assistant needs to feature implementations of design characteristics, that enables the user to not just tolerate but embrace the chaotic nature of voice assistants while still being to unlock their full potential.

## Implications and Outlook
After the thematic analysis, I had to come to terms with the fact that I was not able to continue with the original hypothesis and research question[^11]. The intended weak signals were nowhere to be found. Killing my darlings was half as wild, as the analysis unearthed ample material for further research. I decided to continue working on the second main theme, **Trust Issues**[^12].

Based on the fieldwork done in the last two semesters as well as the literature review, the research now focuses on the particular problem of negotiating privacy with voice assistants. Communication between people relies on multiple modalities and is a multisensory process. Cues are exchanged, eye contact or body posture signalize attention, face and hands add intent. It seems that these cues are missing in the interaction with a voice assistant as it is reduced to just one modality, sound. Voice assistants simulate oral communication but are stripped of other channels of interaction. The hypothesis is given through the thematic analysis. My reformulated research question then would be as follows.

> How can we negotiate trust, respectively privacy, intimacy, and consent with internet of things devices like voice assistants?

The insights I got from the users of voice assistants point to missing cues in the interaction and communication between them and their devices. What effects have the reintroduction of these cues, in regards for the negotiation of privacy between voice assistants and their users? How does a user journey of bonding and developing trust need to be designed? Additional cues could be a stronger visual representation of the state of the assistant to inform the user, as well as using touch, movement, or a beacon-object to signalize certain intents to inform the assistant. 

I would like to answer these questions from a user experience as well as an interaction design perspective. This is in part mirrored by the post-phenomenological approach to technology of D.E. Wittkower and Diane P. Michelfelder [see @wiltseRelatingThingsDesign2020]. In their contributions to Relating to Things, they look at our relationship to privacy as well as voice assistants from the perspective of human experience.

The last phase of prototyping is guided, next to the findings of the analysis and insights so far, by design theory from Jonathan Chapman [@chapmanEmotionallyDurableDesign2015] and Betti Marenko [@marenkoAnimisticDesignHow2016]. Both developed their framework of parameters for the design of relatable technological artifacts that enable bonding and have overlaps in their approaches. I was able to cross-reference and validate these frameworks with studies done by them as well as others.

I plan to use the last semester for designing user journeys that enable bonding and trust. Furthermore, I will implement and test these user journeys on a development platform for voice assistants, built on open-source technologies. In the last few weeks, I was able to set up this development platform as well as implement a handful of first experiences, that play with the aforementioned beacon objects but also with aspects from the guiding frameworks. The tests were successful in so far as showing that the approach would be feasible from a technical point of view.

With this last problem out of the way, I am ready for the design and testing of the user journeys towards bonding and building trust with voice assistants.

[^1]: https://tapt.things.care/researchplan-2020
[^2]: https://tapt.things.care/interview-voice-assistant-basics
[^3]: https://tapt.things.care/participatory-observations
[^4]: https://tapt.things.care/unboxing-the-voice-assistants
[^5]: https://tapt.things.care/expert-workshop-i
[^6]: https://tapt.things.care/reflexive-journal-i
[^7]: https://tapt.things.care/reflexive-journal-ii
[^8]: https://tapt.things.care/reclaim-futures-2020
[^9]: https://tapt.things.care/nerd-2021
[^10]: https://tapt.things.care/finding-of-thematic-analysis
[^11]: https://tapt.things.care/crossroads-and-conjunctions
[^12]: https://tapt.things.care/review-semesters-iii-and-iv
